***SENG 438 - Software Testing, Reliability, and Quality***

**Lab. Report #4 – Mutation Testing and Web app testing**


<table>
  <tr>
   <td><strong>Group #:</strong>
   </td>
   <td>19
   </td>
  </tr>
  <tr>
   <td>Student Names:
   </td>
   <td>Ryan Mailhiot 
   </td>
  </tr>
  <tr>
   <td>
   </td>
   <td>Gabriel Lau 
   </td>
  </tr>
  <tr>
   <td>
   </td>
   <td>Luke Fatovich
   </td>
  </tr>
  <tr>
   <td>
   </td>
   <td>Alex Zhao
   </td>
  </tr>
</table>



## 
**Introduction**

In this lab, we are learning how to perform mutation testing and GUI testing. For mutation testing, we will be analyzing our test suite from the previous assignments for its mutation coverage and attempting to increase it in this assignment using Pitest. For GUI testing, we will be testing a website using Selenium IDE plug-in. We will be designing, executing, and documenting tests for the website.


## 
**Analysis of 10 Mutants of the Range class**



1. Line 91: decremented (--a) local variable number 1.

This mutation caused the output of the Range exception message to be decremented by 1. This caused the exception message to display the lower variable to be 1 less than it actually was. This mutant was not killed in our test suite because we only checked for the existence of the exception, but we never checked the content of the exception message.



2. Line 144: incremented (++a) double local variable number 1.

This mutation caused the lower value of the Range object to be increased by 1 when doing a less than or equal comparison to the contains value. This mutant was not killed by our test suite because we did not test the contains function with an input value that was at the edge of the Range. Thus, despite the lower value being increased by one, the contains function still returned true despite being mutated. We needed to write a test case with a Range length of 1 to kill this mutant.



3. Line 157: greater than to less than

This mutation caused the intersects() function to provide the inverse answer for the first if statement by changing the greater than to less for the comparison of the lower intersect value to the lower range value. This mutant was not killed because despite failing the first if statement, the else statement always still returned true because all of our test cases returned a true response. We needed to have an intersection range below the range object’s range to check for a false return.



4. Line 461: Substituted 32 with -1

This mutation caused the hashCode() to generate a different hash code by swapping the values in the hash calculation. This mutant was not killed because we only made test cases that ensured that hashCode was consistent, not that it returned the specific value with the specific hash calculation.



5. Line 329: removed call to org/jfree/chart/util/ParamChecks::nullNotPermitted

This mutation caused the function to check if the range was null in the expand() function to be removed. This mutant was not killed because we did not write a test case with a null input, thus this null checking function was never called.



6. Line 90: removed conditional - replaced comparison check with false

This mutation caused the lower > upper condition in the Range constructor to be changed to always be false. This mutant was killed because we had a test case for when lower > upper, so this mutant never returned the exception message that would have been expected from the test case, thus killing the mutant.



7. Line 105: Incremented (++a) double field lower

This mutation caused the getLowerBound() function to make the lower variable be 1 higher than intended. This mutant was killed because we had a test case that checked that the getter function returned the exact value when creating a specific Range object.



8. Line 144: Less than to equal

This mutation caused the contains() function to switch the less than comparison to equals when comparing the input value to the lower bound. This mutant was killed because we had a test case with a nominal input value that was in between, but not equal, to the outer bounds of the Range object.



9. Line 161: replaced boolean return with false for org/jfree/data/Range::intersects

This mutation caused the intersects() function to always return false. This mutant was killed by our test suite because we had a test case with nominal input values that expected true results from the function, thus killing the mutant.



10. Line 189: Not equal to equal

This mutation caused the constrain() function to change the conditional !(contains(value) to (contains(value)). This mutant was killed by our test suite because we wrote a test case that does not contain the value, which caused this conditional to be incorrect. This caused the function to return an unexpected result, thus killing the mutant.




## 
**Report all the statistics and the mutation score for each test class**


### Stats for RangeTest


#### Original RangeTest Results





![alt_text](https://github.com/seng438-winter-2023/seng438-a4-LowkeyMadLad/blob/main/media/1-RangeTestPITcoverage.png "RangeTest PIT Coverage")
 \
Pit Test Coverage Report 



![alt_text](https://github.com/seng438-winter-2023/seng438-a4-LowkeyMadLad/blob/main/media/2-RangeTestPITmutations.png "RangeTest PIT Mutations")


Pit Mutations 




![alt_text](https://github.com/seng438-winter-2023/seng438-a4-LowkeyMadLad/blob/main/media/3-RangeTestConsoleTimings.png "RangeTest Console Timings")


Timings on console 


#### New and Improved RangeTest Results





![alt_text](https://github.com/seng438-winter-2023/seng438-a4-LowkeyMadLad/blob/main/media/4-ImprovedRangeTestSummary.png "Improved RangeTest Summary")


Pit Test Summary





![alt_text](https://github.com/seng438-winter-2023/seng438-a4-LowkeyMadLad/blob/main/media/5-ImprovedRangeTestMutations.png "Improved RangeTest Mutations")


Pit Mutations





![alt_text](https://github.com/seng438-winter-2023/seng438-a4-LowkeyMadLad/blob/main/media/6-ImprovedRangeTestTimings.png "Improved RangeTest Timings")


Timings on Console


### Stats for DataUtilitiesTest

It must be noted that we were incapable of increasing the DataUtilitiesTest coverage by 10% and we only increased it by 6%. The reason for this is the fact that our initial mutation coverage was already 85%, which means our coverage was already quite high. Despite writing numerous additional test cases, we could not reach 10%, and could only increase the coverage up to 91%. This is due to the fact that the majority of the remaining 9% mutants are equivalent mutants and are impossible to kill as they do not affect any test cases. 


#### Original DataUtilitiesTest Results




![alt_text](https://github.com/seng438-winter-2023/seng438-a4-LowkeyMadLad/blob/main/media/7-DataUtilitiesTestPITcoverage.png "DataUtilitiesTest PIT Coverage")


Pit Test Summary 





![alt_text](https://github.com/seng438-winter-2023/seng438-a4-LowkeyMadLad/blob/main/media/8-DataUtilitiesTestPITmutations.png "DataUtilitiesTest PIT Mutations")


Pit Mutations 





![alt_text](https://github.com/seng438-winter-2023/seng438-a4-LowkeyMadLad/blob/main/media/9-DataUtilitiesTestConsoleTimings.png "DataUtilitiesTest Console Timings")


Timings on console 


#### New and Improved Results for DataUtilitiesTest





![alt_text](https://github.com/seng438-winter-2023/seng438-a4-LowkeyMadLad/blob/main/media/10-ImprovedDataUtilitiesSummary.png "Improved DataUtilities PIT Coverage")


Pit summary 





![alt_text](https://github.com/seng438-winter-2023/seng438-a4-LowkeyMadLad/blob/main/media/11-ImprovedDataUtilitiesMutations.png "Improved DataUtilities PIT Mutations")


Pit Mutations 





![alt_text](https://github.com/seng438-winter-2023/seng438-a4-LowkeyMadLad/blob/main/media/12-ImprovedDataUtilitiesConsoleTimings.png "Improved DataUtilities Console Timings")


Console Timings 


## 
**Analysis drawn on the effectiveness of each of the test classes**

RangeTest.java 

For this test class we can say that it was fairly effective as our initial results were decently high, with line coverage of 98%, mutation coverage of 65% and test strength at 87%. Since the percentages were fairly high in this case we can say that our testing suite from the previous assignment was effective. But we were able to improve this testing suite over time but it required a lot of tests to be written. As a result we were able to get our line coverage to 100%, mutation coverage to 77% but our test strength decreased to 77%. 

DataUtilities.java

For this test class we can say that it was quite effective as our initial results were very high, with line coverage at 98%, Mutation coverage at 85%, and Test Strength at 87%. Since the percentages were fairly high it can be seen that the test cases that we wrote as a group were effective. In addition to this trying to increase some of these values was also very difficult since the percentage was already very high. As a result we were able to increase our line coverage to 100%, Mutations coverage to 91% and our Test Strength to 91%. 


## 
**A discussion on the effect of equivalent mutants on mutation score accuracy**

Equivalent mutants will lower the mutation score accuracy since some equivalent mutants cannot be killed and due to this the score will either be very high and close to 100. But if you don't consider the equivalent mutants in the overall bugs then the score will be a lot higher, potentially even 100%. In addition to this if we don’t consider the equivalent mutants in the final mutation score accuracy, if a program has 100 mutants, 50 were killed, 5 alive and 45 equivalent mutants then, it looks a little strange and the validity of the accuracy of the mutation score can be up in the air. Since equivalent mutants can either be that it is equivalent to the original program or the test set is inadequate to kill the mutants that are still alive. With this in mind from the example given we have 45 equivalent mutants here some of those mutants are either equivalent to the original program or the test set isn’t adequate enough to kill them. So the overall score of 50/55 which gives 90% overall is questionable since we do not know about the 45 equivalent mutants that survived since they are not considered in the overall test score. 


## 
**A discussion of what could have been done to improve the mutation score of the test suites and our design strategy**

What could have been done to improve mutation score is by obviously looking at the mutation log and seeing where the mutations that survived occurred and then with that information we can write more tests in order to kill those mutations. The way that we designed some of the test cases would be we would look at the report/summary and write tests accordingly. In addition to that we would also write tests that we may have missed in our initial assignment as this also increased the mutation coverage. 

Therefore, for our design strategy we had one member handle each of the test classes. From there, each member ran the Pitest to observe the output of the mutation score. Next, after observing the list of PIT mutations, we looked at which lines of code were showing up the most in the SURVIVED list of mutants. We decided to write test cases that targeted the methods that contained the most SURVIVED mutants in order to increase test coverage as much as possible with the least amount of tests. From there, we designed test cases in an ad hoc manner by simply reading the mutant descriptions and writing test cases to hit these conditions. Next, we simply iterated upon this process by running the Pitest, adding more test cases, and checking the improved coverage until the desired coverage was met.


## How We Identified Equivalent Mutants

The process we followed in order to find equivalent mutants was first by looking at the PIT Mutations report and then by clicking onto it and finding it in the code that was provided. After clicking on that I would then just go into the PIT Summary and find the same code and then look at the mutations in more detail. After that I would try looking at it and determining if it was an equivalent mutant. An example can be seen in the screenshot below :


![alt_text](https://github.com/seng438-winter-2023/seng438-a4-LowkeyMadLad/blob/main/media/13-ExampleMutant.png "Example Mutant")


This is for line 127, the first mutant cannot be killed because it is an equivalent mutant. This is just an example; more similar mutants of this case were found as well.  Some advantages of this process can be that we will be able to find almost all mutants as we are going through all of them individually. Some disadvantages of this are that it may be a long and tedious process as it’ll take a long time depending on how many mutations were missed or how many equivalent mutants are generated. 


### Additional Equivalent Mutants Found in Range

As an example, some other equivalent mutants we found were such as for the Range constructor. There were multiple mutants that incremented the lower and upper value of the Range member variables, but a postfix was used. Therefore, the variables were only incremented/decremented after the local variables were already assigned to the member variables. These are equivalent mutants because the incrementation/decrementation does nothing, as local variables are updated, but then immediately thrown into the garbage collector. Similar problems were found for mutations on getter methods. There were mutants that changed the value in getLowerBound(), getCentralValue() and getUpperBound() AFTER returning the correct output, which led to numerous equivalent mutants.


### Automated Equivalent Mutant Discovery

One way we could discover equivalent mutants is by creating another automated testing program that solely discovers equivalent mutants. The program could simply call the regular functions in the SUT with a set of randomized inputs and then record these inputs and outputs. Next, the program could mutate the methods and then call the same inputs for all of these mutated methods and once again record the new outputs and inputs. From there, the program could simply compare if the output of the methods are different or not. All of the mutated outputs with the same output as the initial unmodified methods, are more likely to be equivalent mutants. From there, a human could possibly inspect these outputs more closely to identify equivalent mutants for certain. This would assume however that a human would have to look over the results to look closer for equivalent mutants, so it is not perfect. The advantages of this process is that is may speed up the process of identifying equivalent mutants, but the disadvantages is that this program would not really be looking at the inner workings of the methods, and solely the inputs and outputs.


## 
**Why do we need mutation testing?**

We need mutation testing since coverage testing doesn’t ensure the overall quality of the testing suite. With mutation testing we can find injected bugs, such as faults that may occur in a program. With mutation testing we can potentially find these bugs and solve these faults in our program before releasing it to a client or out to the public. In addition to this mutation testing can also strengthen the testing suite and thus make it more effective since the tester will have to come up with tests to kill mutants. It also forces the tester to inspect the code that has been written up and which inturn can lead to changes to benefit the code. Overall we need mutation testing as it will improve the testing suite and it will find faults in the source code. 


## 
**Advantages and disadvantages of mutation testing**

Some advantages of mutation testing can be that it will improve the testing suite since it will inject bugs that are similar to faults that may occur in a real program. It can also measure the quality of the tests the programmers have written. It is automated as well which is quite beneficial to programmers. It can also help the testers to check the quality of the source code that was written. Lastly, it can also help with complete code coverage of the source code. 

Some disadvantages can be it may be a tedious process as it may require testers to review the source code and come up with tests. In addition to this the mutation testing softwares like PIT also requires some time in order to give the programmer/tester the summary as seen in this lab. Also for this kind of testing there will be about 4-45% equivalent mutants in about 5000 to 100000 LOC which is alot for a tester. This will also consume a lot of time and resources and normally a manual detection takes about 15 minutes. 


## 
**Explain your SELENUIM test case design process**

The test case design process with Selenium was relatively straightforward on the chrome extension. The functionality that needed to be tested would be first done manually by following the steps you want to reproduce, while Selenium recorded all of the steps you did. It would then allow a repeatable playback of all the steps that were taken. From there we were able to modify the recorded components that Selenium picked up and remove unnecessary steps or add checkpoints and assertions based on the goal of the test. 

 \
The process always started with us finding the element we wanted to test and noting down how to get to it. From there we started the selenium recording, and followed the steps we noted earlier to get to the final test location. We would then add any assertions or checkpoints necessary to complete the test. Following that would be the refinement of tests and making sure that the test works with multiple input data.

We were unable to find defects while testing, however we did find a limitation of Selenium with Amazon and specifically the CSS ::before and ::after tags. Because Selenium uses the document object model to find page components, it cannot read information between ::before and ::after tags. Because of this we could never read changing text on a given page as they are all surrounded in those tags.


## 
**Explain the use of assertions and checkpoints**

Assertions are put in to make sure that the correct element (of any type) is on the website and loading from the correct area. They are very important as a verification tool that a given website has the information that it needs. 

 \
Checkpoints are places on a webpage that can be checked for information such as which page you are on or the logo of the page. They are used as a sort of pass or fail indicator to see if the steps got to the right location. These will generally be different for every page as each page may contain information that makes other usual checkpoints unusable. Common examples of checkpoints are Page titles, Navigation bars, header text fields and static web page components.


## 
**how did you test each functionality with different test data**

For the tests that did not require authentication, it was relatively easy to ensure test data changed for each test. For example the “add item to cart” test does not add the same item to the cart each time and instead adds the item at a given set location which changes hour to hour. This changes the test data depending on when the program is run which is an easy way to test different test data. Tests which involved some sort of querying were easily changed by changing the query and checking that the results were consistent with each unique query. For example the “search item” and “sort by department” tests were created using multiple different searches and departments respectively.


## 
**Discuss advantages and disadvantages of Selenium vs. Sikulix**

Selenium is a tool which accesses the DOM (Document Object Model) of a webpage in order to interact with the different HTML elements of that webpage. It does this through the use of multiple _locators_ which serve as different ways to parse the HTML code (e.g. searching by class, id, name, etc.). SikuliX on the other hand uses image recognition in order to interact with anything that can be displayed to the screen, similar to how a real person would. This has its advantages over Selenium when you want to test a GUI without access to the source code, or where the DOM is hard to obtain or parse using the Selenium locators. It is worth noting that it is also possible to use both tools together as part of a more robust GUI testing framework.

Both the Selenium Web IDE and SikuliX IDE were quite easy to set up and use, and the process of creating GUI tests in either tool was quick and intuitive. The Selenium record feature made each test easy to create by simply performing the test actions manually. Meanwhile, the screenshot feature in SikuliX took a little more time and effort to create tests with, as each image and action would have to be scripted manually one-by-one. Overall, it seems that the ease of creating test cases using Selenium’s record feature makes it the better tool for our use case, since we don’t have any problem accessing the DOM for Amazon’s website for the most part. However, SikuliX could have been used to improve the test suite by allowing us to test the elements we could not access through Selenium alone, such as text embedded within CSS as described earlier.


## 
**How the team work/effort was divided and managed**

The team's work/effort was divided in this way for the assignment, Alex would work on the DataUtilities Mutations, Luke would work on the Range Mutations while Gabe and Ryan would work on the GUI testing portion of this lab. We would then all come back together and discuss our work and then we would also work on the lab report together. For the lab report we would work on our respective sections. If group members had questions about specific sections someone in the group could help that other group member out. 


## 
**Difficulties encountered, challenges overcome, and lessons learned \
**

Some of the difficulties that were encountered at first were trying to get the software to work. This was the case for PIT as some of the instructions were a little confusing to understand. In addition to this there were challenges in increasing the mutation coverage for DataUtilities as our DataUtilities score was fairly high to begin with. Furthermore we also had some trouble increasing the mutation coverage for Range as we found out writing a bunch of tests sometimes and thinking that the mutation coverage would go up due to those tests would be effective but in the end the coverage only went up a few percentages. This was also the case for DataUtilities when writing a bunch of tests we were only to increase the coverage by a little. Another difficulty that occurred at first was knowing when the PIT summary would be generated. We wasted some time restarting the program a few times because we thought the program had crashed or we thought it finished but didn’t do anything. After some time we realized that we just had to let it run before getting the final results. Difficulties that were encountered when doing the GUI testing at first was testing functionalities and use of the software. 

Some challenges we had to overcome were that we had to know when to stop writing tests in order to achieve the 10% increase or any increase at all since running the PIT would take some time. But eventually we were able to get through this challenge and were able to effectively use our time to generate the report, increasing the percentages in the end. As mentioned before we had some trouble with coming up with test for the GUI portion of this assignment and the use of the software but in the end we were able to overcome this difficulty. 

Lessons that we learned in this lab were that this time we were able to split up the work more effectively since the last lab we didn’t. In addition to this as a group we were able to learn more about mutation testing and GUI testing as well through this lab. 


## 
**Comments/feedback on the lab itself**

Some feedback that can be given about this lab can be that the instructions for this lab were a little confusing at first but eventually as a group we were able to figure them out. Also when there were some problems with Eclipse or the setup for this lab there could’ve been a note about needing to import some jar files from assignment two, as just importing the libraries that were given in this zip was not all that was needed or even including those jar files in the zip as well could’ve been helpful.  It can also be stated somewhere an approximate time for how long the PIT summary would take instead of saying that it would take long, as a group we may have misinterpreted this as only being five minutes when in reality it may have taken up to at least twenty minutes of waiting. In addition to this there could’ve been an example of a mutation test and a potential solution to killing a mutant as in the PIT summary trying to understand the mutations were a little confusing at first. 
